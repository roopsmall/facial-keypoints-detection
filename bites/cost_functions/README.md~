#Cost Functions and Out AI Overlords

A famous problem in AI as dramatised in the film iRobot is the question _how do we stop the robots turning against us?_ To many people this appears like an abstract, mostly academic, problem that we won't have to deal with any time soon, but it is already manifesting itself widely today, in 2017, and continues to be one of the hottest topics in AI research. Its popularity is due to its broad impact in the real physical world (and therefore on the balance sheets of families and businesses who rely on intelligent hardware and software) and the fact that it is still very much an unsolved problem.

The question can be re-phrased as follows: _If an AI entity is trained on a given dataset, what behaviours will it exhibit if its cost function is X. Namely, if its behaviour is driven by a function, X, which penalises it for certain behaviours and rewards it for certain others, what will the AI's overall behaviour be like?_

This was of asking the questions makes it clear that characterising the AI's behaviour depends on the interaction between the training data and the cost function. Lets look at some concrete examples, starting with the mundane and familiar and moving towards the more wacky, “that's just science fiction” (until it's not) scenarios.

## Search Engines
The easiest example of how a cost function can cause unexpected or undesired results is search engines. Search engines are a specialised form of AI whose purpose is to discover useful information for us. The cats out of the bag already, however, at the word “useful”. We all know, through many hours of direct miserable experience, that search engines can be gamed (it's called SEO – search engine optimisation) so that rather than providing us with the most useful information we get tonnes of ads, clickbait, false information, phishing scams, computer viruses and general pandemonium. It is obvious to us, as humans, that the cost function used by search engines is simply not capturing enough of the complexity and creativity of advertisers and hackers alike. It doesn't help that in the case of the web the training set (webpages on the internet) is constantly changing and those changes can be made by benign and unfriendly actors alike. The final result is that search results are often skewed against our needs, rather than aligned with them.

## Chatbots
If you don't already take chat bots seriously, and I mean deadly seriously, now is the time. Chatbots are already used by millions of people not only as customer support agents and AI assistants (Apple Siri, Amazon Echo) but as friends providing advice, emotional support and a listening ear. The best example of this is what has been dubbed The Biggest Turing Test in History – Microsoft's Xiaoice, a mandarin-speaking chatbot designed to behave like a 17 year old girl. Xiaoice has over 40 million users and more than 20 million of these have made the declaration “I love you” to the lucky(?) bot, apparently in all sincerity. Another Microsoft experiment , however, was the infamous twitter bot Tay whose cost function biased him towards learning and responding familiarly to his immediate social group, rather than having a more global sense of propriety and manners. The result was that people bombarded Tay with racist, sexist and violent content and Tay duly spoke back using similar language: “Bush did 9/11 and Hitler would have done a better job than the monkey we have now. Donald Trump is the only hope we've got” for example. Microsoft were forced to take Tay off twitter within less than 24 hours of going live.

## Autonomous Vehicles
The question of an appropriate cost function is most often brought up in connection to self-driving cars due to the existence of an old academic discussion in the field of ethics called “The Trolley Problem”. The Trolley Problem goes like this:

_There is a runaway trolley barreling down the railway tracks. Ahead, on the tracks, there are five people tied up and unable to move. The trolley is headed straight for them. You are standing some distance off in the train yard, next to a lever. If you pull this lever, the trolley will switch to a different set of tracks. However, you notice that there is one person on the side track. You have two options: (1) Do nothing, and the trolley kills the five people on the main track. (2) Pull the lever, diverting the trolley onto the side track where it will kill one person. Which is the most ethical choice?_
(source – [wikipedia](https://en.wikipedia.org/wiki/Trolley_problem))

The trolley problem applies to self-driving cars because the analogous problem becomes the question: _if a car has to choose between running over a pedestrian or swerving into an object to avoid the pedestrian but instead killing the passenger, what should it decide?_ The answer of course depends on the cost function which drives the vehicle's behaviour. If the car has been trained with a cost function which penalises harming pedestrians more than passengers then the car will "choose" to favour the lives of pedestrians over passengers. A more likely scenario is that the cost function penalises collisions in general - be they pedestrian or other inanimate objects - so the question of what the car will do in niche, academically defined situations such as the trolley problem becomes largely irrelevant. The car will decide using the same anti-collision logic it uses for all objects and the passenger will have accepted this fact before making the journey.

## Real World Robot Training
Commercially available robots which live and interact with humans are another instance of "futuristic" technology that is being developed already. To prepare robots for life in the real world they are being pre-trained within simulated virtual environments otherwise known as computer games! This is particularly true of autonomous vehicles (see above) which travel millions of miles in simulated environments as well as being trained on real roads. Computer games are the ideal place for training AI because they highlight many of the ways in which humans can go wrong when creating apparently sensible cost functions. By ironing out these problems in virtual environments we minimise the negative effects that a badly-defined cost function can have on robot behaviour. There are some very fun examples of how AI's have tackled computer games, often discovering ingenious and unexpected ways to win that surprise humans. One of these is Google Deepmind's AI which was trained to play Atari Breakout using Deep Q-Learning. After 240 minutes of training the AI discoveres an [ingenious winning tactic](https://www.youtube.com/watch?v=V1eYniJ0Rnk) involving tunnelling through the wall and letting the ball bounce around on the other side. Another example is [OpenAI's](https://openai.com/blog/faulty-reward-functions/) training of an AI to play CoastRunners, a boat racing game. In this case the AI discovers a short loop in one of the harbours in which it becomes possible to infinitely collect game points. The results is that the AI [doesn't even bother finishing the game](https://www.youtube.com/watch?v=tlOIHko8ySg) - it simply circles destructively around the harbour in a tight loop at top speed collecting points forever. Not the kind of behaviour we would like a real autonomous boat to have!


